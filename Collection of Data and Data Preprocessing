Collection of Data and Data Preprocessing

I chose the dataset from Yahoo Finance, feel free to do the same if you need.

#import the required libraries
import pandas as pd
import numpy as np
import matplotlib as mp
import sklearn as sl
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split

#import the file you downloaded
df = pd.read_csv('AAPL.csv')

#check if the file has been uploaded properly
print(df.head())

#find if there is any missing value in the data
if df.isnull().any().any():
    print('Yes, there are missing values')
else:
    print('No, there are no missing values')
    
#find if there are any duplicate values
if df.duplicated().any().any():
    print('Yes, there are duplicate values')
else:
    print('No, there are no duplicate values')
    
#find the categorical variables so that they can be encoded.
categorical_vars = df.dtypes[df.dtypes == 'object'].index
print(df.columns)

#change the values 
df = pd.get_dummies(df, columns=categorical_vars)
print(df.columns)

#The StandardScaler class in scikit-learn standardizes features by removing the mean and scaling to unit variance. 
#The result is a transformed feature where the mean is 0 and the standard deviation is 1. 
#This is a common preprocessing step in machine learning to avoid any bias towards features with larger values.

scaler = StandardScaler()
numerical_vars = list(df.select_dtypes(include=['float64', 'int64']).columns)
df[numerical_vars] = scaler.fit_transform(df[numerical_vars])
print(numerical_vars)

# Define the number of components for PCA
n_components = 2

# Instantiate the PCA object with the number of components
pca = PCA(n_components=n_components)

# Apply PCA to the numerical features of the DataFrame
df_pca = pca.fit_transform(df[numerical_vars])
print(df_pca)

#This splits your DataFrame into X_train (the training features), X_test (the testing features), y_train (the training target), and y_test (the testing target) with a 80:20 ratio.

df['target'] = df['Close'].shift(-1)
X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2)

print(X_train, X_test, y_train, y_test)

#this is end of data preprocessing


#This is the model selection and model training

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

df = pd.read_csv('AAPL.csv')

# Drop unnecessary columns
df = df.drop(['Date', 'Adj Close'], axis=1)

# Normalize the data
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)

# Split the data into features and target
X = df_scaled[:, :-1]
y = df_scaled[:, -1]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Reshape the input data to 3D shape [samples, timesteps, features]
timesteps = 1  # Each sample represents one time step
X_train = X_train.reshape((X_train.shape[0], timesteps, X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0], timesteps, X_test.shape[1]))

model = Sequential()
model.add(LSTM(64, activation='relu', input_shape=(timesteps, X_train.shape[2])))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

loss = model.evaluate(X_test, y_test)
print("Test Loss:", loss)

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

#end of model training

# start of model evaluation

import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mse)

print("Mean Squared Error (MSE):", mse)
print("Mean Absolute Error (MAE):", mae)
print("Root Mean Squared Error (RMSE):", rmse)

import matplotlib.pyplot as plt

# Create a range of indices for the date axis
date_index = range(len(y_test))

# Plotting the predicted and actual stock prices
plt.figure(figsize=(10, 6))
plt.plot(date_index, y_test, label='Actual')
plt.plot(date_index, y_pred, label='Predicted')
plt.title('Actual vs. Predicted Stock Prices')
plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.legend()
plt.show()

import numpy as np
from sklearn.metrics import mean_squared_error

# Define the rolling window size
window_size = 30

# Initialize lists to store evaluation results
mse_scores = []

# Perform rolling window evaluation
for i in range(len(y_test) - window_size + 1):
    window_start = i
    window_end = i + window_size

    # Extract the window of actual and predicted values
    actual_window = y_test[window_start:window_end]
    predicted_window = y_pred[window_start:window_end]

    # Calculate the mean squared error for the window
    mse = mean_squared_error(actual_window, predicted_window)
    mse_scores.append(mse)

# Calculate the average MSE across all windows
average_mse = np.mean(mse_scores)

# Print the average MSE
print("Average MSE: ", average_mse)

